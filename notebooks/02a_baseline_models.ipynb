{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Baseline Models with Class Weights\n",
    "\n",
    "Training LightGBM, CatBoost, XGBoost with class weights (no SMOTE).\n",
    "\n",
    "**Target Metrics:**\n",
    "- F1 >= 0.90\n",
    "- AUC-ROC >= 0.95\n",
    "- FPR < 0.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 800,000, Test: 200,000\n",
      "Features: 61\n",
      "Scale pos weight: 330.7\n"
     ]
    }
   ],
   "source": [
    "# load data from notebook 01\n",
    "with open('../data/processed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "feature_names = data['feature_names']\n",
    "scale_pos = data['scale_pos_weight']\n",
    "\n",
    "print(f\"Train: {len(X_train):,}, Test: {len(X_test):,}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Scale pos weight: {scale_pos:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_proba):\n",
    "    \"\"\"Find threshold that maximizes F1 score.\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    return thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
    "\n",
    "def evaluate_model(y_true, y_proba, threshold=None):\n",
    "    \"\"\"Evaluate model with optimal or given threshold.\"\"\"\n",
    "    if threshold is None:\n",
    "        threshold = find_optimal_threshold(y_true, y_proba)\n",
    "    \n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'auc': roc_auc_score(y_true, y_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_proba),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'fpr': fp / (fp + tn),\n",
    "        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.0586014\n",
      "[200]\tvalid_0's binary_logloss: 0.0234222\n",
      "[300]\tvalid_0's binary_logloss: 0.0126453\n",
      "[400]\tvalid_0's binary_logloss: 0.0087105\n",
      "[500]\tvalid_0's binary_logloss: 0.00730667\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's binary_logloss: 0.00730667\n",
      "\n",
      "LightGBM (13.6s):\n",
      "  AUC: 0.9425\n",
      "  F1: 0.8086\n",
      "  Precision: 0.9559\n",
      "  Recall: 0.7007\n",
      "  FPR: 0.0095%\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    num_leaves=128,\n",
    "    learning_rate=0.03,\n",
    "    scale_pos_weight=scale_pos,\n",
    "    feature_fraction=0.7,\n",
    "    bagging_fraction=0.7,\n",
    "    bagging_freq=5,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "lgb_time = time.time() - start\n",
    "\n",
    "lgb_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "lgb_results = evaluate_model(y_test, lgb_proba)\n",
    "\n",
    "print(f\"\\nLightGBM ({lgb_time:.1f}s):\")\n",
    "print(f\"  AUC: {lgb_results['auc']:.4f}\")\n",
    "print(f\"  F1: {lgb_results['f1']:.4f}\")\n",
    "print(f\"  Precision: {lgb_results['precision']:.4f}\")\n",
    "print(f\"  Recall: {lgb_results['recall']:.4f}\")\n",
    "print(f\"  FPR: {lgb_results['fpr']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6635900\ttest: 0.6643687\tbest: 0.6643687 (0)\ttotal: 84ms\tremaining: 41.9s\n",
      "100:\tlearn: 0.2299621\ttest: 0.2781862\tbest: 0.2780603 (99)\ttotal: 3.1s\tremaining: 12.2s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.2713556063\n",
      "bestIteration = 142\n",
      "\n",
      "Shrink model to first 143 iterations.\n",
      "\n",
      "CatBoost (6.1s):\n",
      "  AUC: 0.9570\n",
      "  F1: 0.8689\n",
      "  Precision: 0.9934\n",
      "  Recall: 0.7721\n",
      "  FPR: 0.0015%\n"
     ]
    }
   ],
   "source": [
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3,\n",
    "    class_weights=[1, scale_pos],\n",
    "    random_seed=42,\n",
    "    verbose=100,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "cat_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "cat_time = time.time() - start\n",
    "\n",
    "cat_proba = cat_model.predict_proba(X_test)[:, 1]\n",
    "cat_results = evaluate_model(y_test, cat_proba)\n",
    "\n",
    "print(f\"\\nCatBoost ({cat_time:.1f}s):\")\n",
    "print(f\"  AUC: {cat_results['auc']:.4f}\")\n",
    "print(f\"  F1: {cat_results['f1']:.4f}\")\n",
    "print(f\"  Precision: {cat_results['precision']:.4f}\")\n",
    "print(f\"  Recall: {cat_results['recall']:.4f}\")\n",
    "print(f\"  FPR: {cat_results['fpr']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.87032\n",
      "[100]\tvalidation_0-auc:0.96099\n",
      "[200]\tvalidation_0-auc:0.96223\n",
      "[271]\tvalidation_0-auc:0.96147\n",
      "\n",
      "XGBoost (11.6s):\n",
      "  AUC: 0.9626\n",
      "  F1: 0.8755\n",
      "  Precision: 0.9831\n",
      "  Recall: 0.7891\n",
      "  FPR: 0.0040%\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    scale_pos_weight=scale_pos,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=100)\n",
    "xgb_time = time.time() - start\n",
    "\n",
    "xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "xgb_results = evaluate_model(y_test, xgb_proba)\n",
    "\n",
    "print(f\"\\nXGBoost ({xgb_time:.1f}s):\")\n",
    "print(f\"  AUC: {xgb_results['auc']:.4f}\")\n",
    "print(f\"  F1: {xgb_results['f1']:.4f}\")\n",
    "print(f\"  Precision: {xgb_results['precision']:.4f}\")\n",
    "print(f\"  Recall: {xgb_results['recall']:.4f}\")\n",
    "print(f\"  FPR: {xgb_results['fpr']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINE RESULTS\n",
      "============================================================\n",
      "          AUC-ROC  PR-AUC      F1  Precision  Recall     FPR  Threshold\n",
      "LightGBM   0.9425  0.7430  0.8086     0.9559  0.7007  0.0001     0.3463\n",
      "CatBoost   0.9570  0.8155  0.8689     0.9934  0.7721  0.0000     0.5340\n",
      "XGBoost    0.9626  0.8269  0.8755     0.9831  0.7891  0.0000     0.4703\n",
      "\n",
      "Best model: XGBoost (F1 = 0.8755)\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'LightGBM': lgb_results,\n",
    "    'CatBoost': cat_results,\n",
    "    'XGBoost': xgb_results\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    name: {\n",
    "        'AUC-ROC': r['auc'],\n",
    "        'PR-AUC': r['pr_auc'],\n",
    "        'F1': r['f1'],\n",
    "        'Precision': r['precision'],\n",
    "        'Recall': r['recall'],\n",
    "        'FPR': r['fpr'],\n",
    "        'Threshold': r['threshold']\n",
    "    }\n",
    "    for name, r in results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.round(4))\n",
    "\n",
    "best = comparison['F1'].idxmax()\n",
    "print(f\"\\nBest model: {best} (F1 = {comparison.loc[best, 'F1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 Features:\n",
      "                      feature  importance\n",
      "                       amount   18.772846\n",
      "         amount_vs_mean_ratio   18.752379\n",
      "                   amount_log   14.883606\n",
      "               amount_sum_24h   13.653489\n",
      "                is_high_value    3.332770\n",
      "        ratio_to_customer_max    2.489376\n",
      "               velocity_score    2.200832\n",
      "                        month    1.673035\n",
      "            amount_mean_total    1.485194\n",
      "              channel_encoded    1.370542\n",
      "              amount_change_1    1.207332\n",
      "amount_deviation_from_rolling    1.106391\n",
      "                    month_cos    1.010487\n",
      "                    month_sin    0.997927\n",
      "                amount_std_7d    0.961910\n"
     ]
    }
   ],
   "source": [
    "# feature importance\n",
    "importance = cat_model.get_feature_importance()\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Features:\")\n",
    "print(feat_imp.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved to ../models/\n"
     ]
    }
   ],
   "source": [
    "# save models and results\n",
    "baseline_data = {\n",
    "    'models': {\n",
    "        'lightgbm': lgb_model,\n",
    "        'catboost': cat_model,\n",
    "        'xgboost': xgb_model\n",
    "    },\n",
    "    'predictions': {\n",
    "        'lightgbm': lgb_proba,\n",
    "        'catboost': cat_proba,\n",
    "        'xgboost': xgb_proba\n",
    "    },\n",
    "    'results': results,\n",
    "    'best_model': best\n",
    "}\n",
    "\n",
    "with open('../models/baseline_models.pkl', 'wb') as f:\n",
    "    pickle.dump(baseline_data, f)\n",
    "\n",
    "# also save CatBoost separately\n",
    "cat_model.save_model('../models/catboost_baseline.cbm')\n",
    "\n",
    "print(\"Models saved to ../models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
